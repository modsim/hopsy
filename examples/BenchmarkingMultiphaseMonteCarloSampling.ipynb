{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Benchmarking Multiphase Monte Carlo Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Motivation\n",
    "\n",
    "As a methods researcher one is often interested in testing new methods and benchmarking them together with state-of-the-art algorithms.\n",
    "hopsy provides the framework for conducting such comparisons, because it allows the methods researcher to focus on the algorithms, while the scaffolding, i.e., I/O, convergence diagnostics, and state-of-the-art implementations of existing algorithms are provided.\n",
    "\n",
    "\n",
    "We show in this notebook how one can adapt build a multiphase monte carlo sampling plugin for hopsy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Background\n",
    "Instead of rounding before sampling, multiphase monte carlo sampling rounds adaptively based on the samples of the Markov chain during a run.\n",
    "It has been reported to be an efficient strategy, if the underlying Markov chain is efficient enough.\n",
    "\n",
    "The Idea has found previous adaption in the artificially centered coordinate hit-and-run and the optGP sampler.\n",
    "However, Coordinate Hit-and-Run Rounded showed greater and more stable performance (https://pubmed.ncbi.nlm.nih.gov/28158334/) than both of those algorithms.\n",
    "\n",
    "The idea is resurrected and shown to be efficient given a better Markov chain implementation.\n",
    "See https://drops.dagstuhl.de/opus/volltexte/2021/13820/pdf/LIPIcs-SoCG-2021-21.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hopsy\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import time\n",
    "import PolyRound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom proposals\n",
    "In case the proposal you want to benchmark is not already implemented, it is possible \n",
    "to implement it yourself in a small class. It is then possible to seamlessy integrate this proposal\n",
    "into existing workflows with hopsy. As an example, we implement\n",
    "Overrelaxed Hit-and-Run, see https://ui.adsabs.harvard.edu/abs/2015IJMPC..2650010D/abstract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OverrelaxedHitAndRunProposal:\n",
    "    def __init__(\n",
    "        self, problem: hopsy.Problem, starting_point: np.ndarray, epsilon: float = 1e-10\n",
    "    ):\n",
    "        self.A = problem.A\n",
    "        self.b = problem.b\n",
    "        self.state = starting_point\n",
    "        self.proposal = self.state\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def propose(self, rng) -> np.ndarray:\n",
    "        \"\"\"This method is called by hopsy.MarkovChain. Returns the proposal\"\"\"\n",
    "        direction = np.random.randn(self.state.shape[0])\n",
    "        direction /= np.linalg.norm(direction, axis=0)\n",
    "\n",
    "        inverse_distances = np.nan_to_num(\n",
    "            np.divide(\n",
    "                self.A @ direction, self.b - self.A @ self.state, self.A @ direction\n",
    "            )\n",
    "        )\n",
    "        forward_distance = 1.0 / np.max(inverse_distances)\n",
    "        backward_distance = 1.0 / np.min(inverse_distances)\n",
    "\n",
    "        L = forward_distance - backward_distance\n",
    "        a = 2 * (-backward_distance) / L - 1\n",
    "        if a == 0:\n",
    "            a = self.epsilon\n",
    "        t_1 = (\n",
    "            L * (1 + a - np.sqrt((1 + a) ** 2 - 4 * a * np.random.uniform())) / (2 * a)\n",
    "        )\n",
    "        t_0 = -backward_distance\n",
    "\n",
    "        self.proposal = self.state + (t_1 - t_0) * direction\n",
    "        return self.proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More complex custom sampling schemes\n",
    "In addition to custom proposals, it is also possible to implement adaptive workflows.\n",
    "In this case we implement the adaptive rounding from https://drops.dagstuhl.de/opus/volltexte/2021/13820/pdf/LIPIcs-SoCG-2021-21.pdf using the Billiard walk that is already implemented in hopsy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rounding based on samples, as suggested in https://drops.dagstuhl.de/opus/volltexte/2021/13820/pdf/LIPIcs-SoCG-2021-21.pdf\n",
    "def svd_rounding(samples, polytope):\n",
    "    # We concatenate them samples to [n_dim, n_iterations] for rounding\n",
    "    stacked_samples = np.vstack(\n",
    "        (samples[0, :, :], samples[1, :, :], samples[2, :, :], samples[3, :, :])\n",
    "    )\n",
    "\n",
    "    mean = np.mean(stacked_samples, axis=0)\n",
    "    stacked_samples = stacked_samples - mean\n",
    "    U, S, Vh = np.linalg.svd(stacked_samples)\n",
    "    # Rescaling as mentioned in  https://drops.dagstuhl.de/opus/volltexte/2021/13820/pdf/LIPIcs-SoCG-2021-21.pdf\n",
    "    s_ratio = np.max(S) / np.min(S)\n",
    "    S = S / np.min(S)\n",
    "    if np.max(S) >= 2.0:\n",
    "        S[np.where(S < 2.0)] = 1.0\n",
    "    else:\n",
    "        S = np.ones(S.shape)\n",
    "        Vh = np.identity(Vh.shape[0])\n",
    "\n",
    "    rounding_matrix = np.transpose(Vh).dot(np.diag(S))\n",
    "\n",
    "    # Transforms current last samples into new polytope\n",
    "    sub_problem = hopsy.Problem(\n",
    "        polytope.A, polytope.b, transformation=rounding_matrix, shift=mean\n",
    "    )\n",
    "    starting_points = hopsy.transform(\n",
    "        sub_problem, [samples[i, -1, :] for i in range(samples.shape[0])]\n",
    "    )\n",
    "    polytope.apply_shift(mean)\n",
    "    polytope.apply_transformation(rounding_matrix)\n",
    "\n",
    "    return s_ratio, starting_points, polytope, sub_problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "In this section we implement some helper functions for executing the benchmarks as well as a function\n",
    "which constructs our benchmark problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions for calling the sampling algorithms and defining the polytopes\n",
    "def run_multiphase_sampling(\n",
    "    proposal,\n",
    "    polytope,\n",
    "    seeds: List,\n",
    "    target_ess: float,\n",
    "    steps_per_phase: int,\n",
    "    starting_points: List,\n",
    "):\n",
    "    limit_singular_ratio_value = 2.3  # from https://drops.dagstuhl.de/opus/volltexte/2021/13820/pdf/LIPIcs-SoCG-2021-21.pdf\n",
    "    assert len(starting_points) == len(seeds)\n",
    "    rngs = [hopsy.RandomNumberGenerator(s) for s in seeds]\n",
    "\n",
    "    ess = 0\n",
    "    iterations = 0\n",
    "    s_ratio = limit_singular_ratio_value + 1\n",
    "    sampling_time = 0\n",
    "    samples = None\n",
    "    last_iteration_did_rounding = True\n",
    "    while ess < target_ess:\n",
    "        iterations += 1\n",
    "        print(\"\\t\\titer:\", iterations)\n",
    "        internal_polytope = polytope\n",
    "        p = hopsy.Problem(internal_polytope.A, internal_polytope.b)\n",
    "        markov_chains = [\n",
    "            hopsy.MarkovChain(proposal=proposal, problem=p, starting_point=s)\n",
    "            for s in starting_points\n",
    "        ]\n",
    "\n",
    "        start = time.time()\n",
    "        acceptance_rate, _samples = hopsy.sample(\n",
    "            markov_chains, rngs, n_samples=steps_per_phase, thinning=1, n_procs=1\n",
    "        )\n",
    "        end = time.time()\n",
    "        assert 0 < np.min(p.b - p.A @ _samples[0, -1, :])\n",
    "        assert 0 < np.min(p.b - p.A @ _samples[1, -1, :])\n",
    "        assert 0 < np.min(p.b - p.A @ _samples[2, -1, :])\n",
    "        assert 0 < np.min(p.b - p.A @ _samples[3, -1, :])\n",
    "        sampling_time += end - start\n",
    "\n",
    "        if s_ratio > limit_singular_ratio_value:\n",
    "            samples = _samples\n",
    "            # also measures the rounding time\n",
    "            start = time.time()\n",
    "            s_ratio, starting_points, internal_polytope, sub_problem = svd_rounding(\n",
    "                samples, internal_polytope\n",
    "            )\n",
    "            end = time.time()\n",
    "            sampling_time += end - start\n",
    "            print(\"\\t\\ts_ratio:\", s_ratio)\n",
    "            last_iteration_did_rounding = True\n",
    "        else:\n",
    "            if last_iteration_did_rounding:\n",
    "                # next operation transforms last samples to current space before concatenating\n",
    "                for j in range(samples.shape[0]):\n",
    "                    samples[j] = hopsy.transform(\n",
    "                        sub_problem, [samples[j, i, :] for i in range(samples.shape[1])]\n",
    "                    )\n",
    "                last_iteration_did_rounding = False\n",
    "\n",
    "            samples = np.concatenate((samples, _samples), axis=1)\n",
    "            starting_points = [samples[i, -1, :] for i in range(samples.shape[0])]\n",
    "        ess = np.min(hopsy.ess(samples))\n",
    "        print(\"\\t\\tess\", str(ess))\n",
    "        steps_per_phase += 100\n",
    "\n",
    "    # transforms back to full space\n",
    "    _samples = np.zeros(\n",
    "        (samples.shape[0], samples.shape[1], polytope.transformation.shape[0])\n",
    "    )\n",
    "    for j in range(samples.shape[0]):\n",
    "        _samples[j] = hopsy.back_transform(\n",
    "            hopsy.Problem(\n",
    "                A=internal_polytope.A,\n",
    "                b=internal_polytope.b,\n",
    "                transformation=internal_polytope.transformation,\n",
    "                shift=internal_polytope.shift,\n",
    "            ),\n",
    "            [samples[j, i, :] for i in range(samples.shape[1])],\n",
    "        )\n",
    "\n",
    "    return _samples, iterations, ess, sampling_time\n",
    "\n",
    "\n",
    "def run_sampling(\n",
    "    proposal,\n",
    "    polytope,\n",
    "    seeds: List,\n",
    "    target_ess: float,\n",
    "    steps_per_phase: int,\n",
    "    starting_points: List,\n",
    "    thinning=None,\n",
    "):\n",
    "    rngs = [hopsy.RandomNumberGenerator(s) for s in seeds]\n",
    "\n",
    "    problem = hopsy.Problem(polytope.A, polytope.b)\n",
    "    thinning = (\n",
    "        thinning if thinning is not None else int(float(problem.A.shape[1] ** 2) / 6)\n",
    "    )\n",
    "    ess = 0\n",
    "    iterations = 0\n",
    "    # also measures the rounding time\n",
    "    start = time.time()\n",
    "    rounded_problem = hopsy.round(problem)\n",
    "    # transforms starting points into rounded space\n",
    "    starting_points = hopsy.transform(rounded_problem, starting_points)\n",
    "    end = time.time()\n",
    "    sampling_time = end - start\n",
    "    samples = None\n",
    "    markov_chains = [\n",
    "        hopsy.MarkovChain(problem=rounded_problem, starting_point=s)\n",
    "        for s in starting_points\n",
    "    ]\n",
    "    while ess < target_ess:\n",
    "        iterations += 1\n",
    "        print(\"\\t\\titer:\", iterations)\n",
    "        for i in range(len(starting_points)):\n",
    "            markov_chains[i].proposal = proposal(\n",
    "                rounded_problem, starting_point=starting_points[i]\n",
    "            )\n",
    "\n",
    "        start = time.time()\n",
    "        acceptance_rate, _samples = hopsy.sample(\n",
    "            markov_chains, rngs, n_samples=steps_per_phase, thinning=thinning, n_procs=1\n",
    "        )\n",
    "        end = time.time()\n",
    "        sampling_time += end - start\n",
    "\n",
    "        samples = (\n",
    "            _samples if samples is None else np.concatenate((samples, _samples), axis=1)\n",
    "        )\n",
    "        starting_points = hopsy.transform(rounded_problem, samples[:, -1, :])\n",
    "\n",
    "        ess = np.min(hopsy.ess(samples))\n",
    "        print(\"\\t\\tess\", str(ess) + \",\", \"samples\", samples.shape[1])\n",
    "\n",
    "    # transforms back to full space\n",
    "    _samples = np.zeros(\n",
    "        (samples.shape[0], samples.shape[1], polytope.transformation.shape[0])\n",
    "    )\n",
    "    for j in range(samples.shape[0]):\n",
    "        _samples[j] = hopsy.back_transform(\n",
    "            hopsy.Problem(\n",
    "                A=polytope.A,\n",
    "                b=polytope.b,\n",
    "                transformation=polytope.transformation,\n",
    "                shift=polytope.shift,\n",
    "            ),\n",
    "            [samples[j, i, :] for i in range(samples.shape[1])],\n",
    "        )\n",
    "    return _samples, iterations, ess, sampling_time\n",
    "\n",
    "\n",
    "def generate_polytope(name):\n",
    "    if name == \"BP5\":\n",
    "        bp = hopsy.BirkhoffPolytope(5)\n",
    "        polytope = PolyRound.mutable_classes.polytope.Polytope(A=bp.A, b=bp.b)\n",
    "        polytope.normalize()\n",
    "        parameter_names = [\"x\" + str(c) for c in polytope.A.columns]\n",
    "        return polytope, \"5D Birkhoff Polytope\", parameter_names\n",
    "    elif name == \"e_coli\":\n",
    "        original_polytope = PolyRound.static_classes.parse_sbml_stoichiometry.StoichiometryParser.parse_sbml_cobrapy(\n",
    "            \"../extern/hops/resources/e_coli_core/e_coli_core.xml\"\n",
    "        )\n",
    "        parameter_names = original_polytope.A.columns\n",
    "        polytope = PolyRound.api.PolyRoundApi.simplify_polytope(original_polytope)\n",
    "        polytope = PolyRound.api.PolyRoundApi.transform_polytope(polytope)\n",
    "        polytope.normalize()\n",
    "        return polytope, \"e_coli_core\", parameter_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark parameters\n",
    "\n",
    "Next set up the benchmark parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_ess = 1000\n",
    "proposalTypes = {\n",
    "    \"Billiard walk\": hopsy.BilliardWalkProposal,\n",
    "    \"CHRRT\": hopsy.UniformCoordinateHitAndRunProposal,\n",
    "    # \"OHRR\": OverrelaxedHitAndRunProposal,\n",
    "}\n",
    "seeds = [1, 2, 3, 4]\n",
    "\n",
    "problems_to_benchmark = [\"BP5\", \"e_coli\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "Sample all problems with all sampling schemes, i.e., two for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking {problem_selection}\n",
      "\tproposal Billiard walk\n",
      "\t\titer: 1\n",
      "\t\ts_ratio: 1.3847452299003389\n",
      "\t\tess 231.4835868364245\n",
      "\t\titer: 2\n",
      "\t\tess 627.6793244654275\n",
      "\t\titer: 3\n",
      "\t\tess 990.5612889618237\n",
      "\t\titer: 4\n",
      "\t\tess 1583.2881720652492\n",
      "\tproposal CHRRT\n",
      "\t\titer: 1\n",
      "\t\tess 453.9004456107389, samples 320\n",
      "\t\titer: 2\n",
      "\t\tess 894.4609120329771, samples 640\n",
      "\t\titer: 3\n",
      "\t\tess 1321.6148361675992, samples 960\n",
      "\tBilliard walk rhat: 1.0061219415504485\n",
      "\tCHRRT rhat: 1.0038641732435272\n",
      "\tess {'Billiard walk': 1583.2881720652492, 'CHRRT': 1321.6148361675992}\n",
      "\ttimes {'Billiard walk': 0.06513357162475586, 'CHRRT': 0.9482927322387695}\n",
      "\tess/t {'Billiard walk': 24308.327220051844, 'CHRRT': 1393.678124103593}\n",
      "Benchmarking {problem_selection}\n",
      "\tproposal Billiard walk\n",
      "\t\titer: 1\n",
      "\t\ts_ratio: 11.963156850679272\n",
      "\t\tess 4.872243577042705\n",
      "\t\titer: 2\n",
      "\t\ts_ratio: 23.96780496221371\n",
      "\t\tess 5.133013163411473\n",
      "\t\titer: 3\n",
      "\t\ts_ratio: 33.92514821626017\n",
      "\t\tess 7.03213882586069\n",
      "\t\titer: 4\n",
      "\t\ts_ratio: 23.48072122188601\n",
      "\t\tess 21.114165738738418\n",
      "\t\titer: 5\n",
      "\t\ts_ratio: 2.3472103713065406\n",
      "\t\tess 652.8113104108883\n",
      "\t\titer: 6\n",
      "\t\ts_ratio: 2.1466424838886535\n",
      "\t\tess 952.7823528588415\n",
      "\t\titer: 7\n",
      "\t\tess 2089.437296408794\n",
      "\tproposal CHRRT\n",
      "\t\titer: 1\n",
      "\t\tess 508.6356364335463, samples 480\n",
      "\t\titer: 2\n",
      "\t\tess 1079.4569141209104, samples 960\n",
      "\tBilliard walk rhat: 1.002332053243373\n",
      "\tCHRRT rhat: 1.0106888666904759\n",
      "\tess {'Billiard walk': 2089.437296408794, 'CHRRT': 1079.4569141209104}\n",
      "\ttimes {'Billiard walk': 1.2897939682006836, 'CHRRT': 0.9892332553863525}\n",
      "\tess/t {'Billiard walk': 1619.9775684512201, 'CHRRT': 1091.205646639245}\n"
     ]
    }
   ],
   "source": [
    "samples = {}\n",
    "iterations = {}\n",
    "ess = {}\n",
    "times = {}\n",
    "ess_t = {}\n",
    "\n",
    "for problem_selection in problems_to_benchmark:\n",
    "    print(\"Benchmarking {problem_selection}\")\n",
    "    samples[problem_selection] = {}\n",
    "    iterations[problem_selection] = {}\n",
    "    ess[problem_selection] = {}\n",
    "    times[problem_selection] = {}\n",
    "    ess_t[problem_selection] = {}\n",
    "    for name, p in proposalTypes.items():\n",
    "        print(\"\\tproposal\", name)\n",
    "        # resets problem and starting points\n",
    "        preprocessed_polytope, problem_name, parameter_names = generate_polytope(\n",
    "            problem_selection\n",
    "        )\n",
    "        steps_per_phase = preprocessed_polytope.A.shape[1] * 20\n",
    "        cheby = hopsy.compute_chebyshev_center(\n",
    "            hopsy.Problem(A=preprocessed_polytope.A, b=preprocessed_polytope.b)\n",
    "        ).flatten()\n",
    "        starting_points = [cheby for s in seeds]\n",
    "\n",
    "        if name == \"Billiard walk\":\n",
    "            (\n",
    "                samples[problem_selection][name],\n",
    "                iterations[problem_selection][name],\n",
    "                ess[problem_selection][name],\n",
    "                times[problem_selection][name],\n",
    "            ) = run_multiphase_sampling(\n",
    "                proposal=p,\n",
    "                polytope=preprocessed_polytope,\n",
    "                seeds=seeds,\n",
    "                target_ess=target_ess,\n",
    "                steps_per_phase=steps_per_phase,\n",
    "                starting_points=starting_points,\n",
    "            )\n",
    "        else:\n",
    "            thinning = (\n",
    "                1\n",
    "                if name != \"CHRRT\"\n",
    "                else max(int(float(preprocessed_polytope.A.shape[1] ** 2) / 6), 1)\n",
    "            )\n",
    "            samples[problem_selection][name], iterations[problem_selection][name], ess[problem_selection][name], times[problem_selection][name] = run_sampling(\n",
    "                proposal=p,\n",
    "                polytope=preprocessed_polytope,\n",
    "                seeds=seeds,\n",
    "                target_ess=target_ess,\n",
    "                steps_per_phase=steps_per_phase,\n",
    "                starting_points=starting_points,\n",
    "                thinning=thinning,\n",
    "            )\n",
    "        ess_t[problem_selection][name] = ess[problem_selection][name] / times[problem_selection][name]\n",
    "\n",
    "    # check convergence\n",
    "    for name, s in samples[problem_selection].items():\n",
    "        print(\"\\t\" + name, \"rhat:\", np.max(hopsy.rhat(s)))\n",
    "    \n",
    "    print(\"\\tess\", ess[problem_selection])\n",
    "    print(\"\\ttimes\", times[problem_selection])\n",
    "    print(\"\\tess/t\", ess_t[problem_selection])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
